# Week 3 - Decision Trees, Boosting, PCA, Feature Selection
 - One and only playlist you will need to build upon tree/boosting based models - [Statquest](https://www.youtube.com/watch?v=_L39rN6gz7Y&list=PLakf6XoaFMqKLBIkOTGxUgABAeNPmbv9g)
 -  [visualization](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) of trees
 -  [Piyush rai slides](https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec3_slides.pdf) on decision trees, goes into maths.
 -  What is [boosting](https://medium.com/@brijesh_soni/understanding-boosting-in-machine-learning-a-comprehensive-guide-bdeaa1167a6)?
 -  [Boosting vs Bagging](https://www.geeksforgeeks.org/bagging-vs-boosting-in-machine-learning/)
 -  [Catboost](https://youtu.be/KXOTSkPL2X4?si=kwL7Ij4k0U46Uejd), watch its part 2 also
 -  for surface level idea of the above - [medium article]https://medium.com/analytics-vidhya/tree-based-machine-learning-algorithms-explained-b50937d3cf8e
 -  [PCA](https://youtu.be/FgakZw6K1QQ?si=in5c9XHRWwGwLh5p) by statquest(GOAT!!), [Piyush rai](https://cse.iitk.ac.in/users/piyush/courses/ml_autumn16/771A_lec11_slides.pdf) on PCA
 -  [Shapley scores](https://youtu.be/NBg7YirBTN8?si=ZclVf-fmX16wq9pY), a good [read](https://christophm.github.io/interpretable-ml-book/shapley.html)
 -  [RFE](https://machinelearningmastery.com/rfe-feature-selection-in-python/)
 

## Extras -
 -  A very good [book](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf) on core ml topics, can start reading this to build a very strong foundation in ml (should've shared this earlier).
 - Another good books - [ISLR](https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6009dd9fa7bc363aa822d2c7/1611259312432/ISLR+Seventh+Printing.pdf) and the [ESLR](https://hastie.su.domains/ElemStatLearn/). You can go through the above topic content from ISLR. This part is not expected to be covered in a week, you can start studying it and gradually understand the algos from a mathematical pov, but again you dont need this much for implementation ;)
 
## Week 3 Assignment
This week you will be working with decision trees, boosting algorithms, PCA, and feature selection techniques.

Generate at least 10-20 relevant features, check the correlation between them, and then use PCA to reduce the dimensionality of your dataset. You will also now implement decision trees and boosting algorithms like XGBoost for prediction - notice the change in performance from last week.

### Tasks
1. **Decision Trees**: Implement a decision tree model using the dataset provided.
2. **Boosting Algorithms**: Implement boosting algorithms like XGBoost or AdaBoost on the dataset.
3. **PCA**: Apply PCA to reduce the dimensionality of your dataset and visualize the results.
4. **Feature Selection**: Use techniques like Recursive Feature Elimination (RFE) to select the most important features from your dataset.
5. **SHAP Explainer**: Use SHAP (SHapley Additive exPlanations) to interpret the predictions of your models.

